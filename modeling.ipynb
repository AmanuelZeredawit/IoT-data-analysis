{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split\n",
    "To avoid overfitting, it's common practice in Machine Learning to split data into train and test datasets. This is done to ensure that the model is able to correctly predict new, unseen data.\n",
    "\n",
    "Since we're working with time-series data, we cannot use random split methods, as that would allow the model to know the future.\n",
    "\n",
    "A function to print the start and end of a DataFrame is available as show_start_end(), which takes a DataFrame as the only argument, and returns a string.\n",
    "\n",
    "The data is available as environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the split day\n",
    "limit_day = \"2018-10-27\"\n",
    "\n",
    "# Split the data\n",
    "train_env = environment[:limit_day]\n",
    "test_env = environment[limit_day:]\n",
    "\n",
    "# Print start and end dates\n",
    "print(show_start_end(train_env))\n",
    "print(show_start_end(test_env))\n",
    "\n",
    "# Split the data into X and y\n",
    "X_train = train_env.drop('target', axis=1)\n",
    "y_train = train_env['target']\n",
    "X_test = test_env.drop('target', axis=1)\n",
    "y_test = test_env['target']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Using the data from the previous exercise, you'll now train a Machine learning model.\n",
    "\n",
    "In line with best practices, the data is now available as X_train, while the labels have been loaded as y_train. A subset of the data is also available as X_test. You'll learn later in this chapter how to properly create these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict classes\n",
    "print(logreg.predict(X_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance\n",
    "You're now going to evaluate the model from the previous lesson against the test-data.\n",
    "\n",
    "Evaluating data against new, unseen data is important, as it proves the ability of the model to correctly estimate data it has never encountered before.\n",
    "\n",
    "All necessary modules have been imported, and the data is available as X_train and y_train, and X_test and y_test respectively.\n",
    "\n",
    "Instructions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LogisticRegression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Score the model\n",
    "\n",
    "print(logreg.score(X_train,y_train))\n",
    "print(logreg.score(X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "Before applying a machine learning algorithm, one of the most common operations applied to the data is scaling.\n",
    "\n",
    "Scaling data helps the algorithm converge faster. It also avoids having one feature dominate all other features.\n",
    "\n",
    "You'll now create and inspect a standard scaler object.\n",
    "\n",
    "The data is available as environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "sc.fit(environment)\n",
    "\n",
    "# Transform the data\n",
    "environ_scaled = sc.transform(environment)\n",
    "\n",
    "# Convert scaled data to DataFrame\n",
    "environ_scaled = pd.DataFrame(environ_scaled, \n",
    "                              columns=environment.columns, \n",
    "                              index= environment.index)\n",
    "print(environ_scaled.head())\n",
    "plot_unscaled_scaled(environment, environ_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Pipelines\n",
    "You'll now use one of the best features scikit-learn has to offer, Pipelines. Pipelines allow you to chain multiple actions, like transformations and estimations, which are applied sequentially to new data.\n",
    "\n",
    "You'll now create a pipeline containing both a StandardScaler and a LogisticRegression estimator.\n",
    "\n",
    "This allows you to pass unscaled data to the pipeline, where the Scaler will scale the data, and the LogisticRegression will predict the target column.\n",
    "\n",
    "The unscaled data is available as X_train, while the labels have been loaded as y_train. A subset of the data, X_test , is also available to evaluate the model.\n",
    "\n",
    "StandardScaler and LogisticRegression have been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create Scaler and Regression objects\n",
    "sc = StandardScaler()\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Create Pipeline\n",
    "pl = Pipeline([\n",
    "        (\"scale\", sc),\n",
    "        (\"logreg\", logreg)\n",
    "    ])\n",
    "\n",
    "# Fit the pipeline and print predictions\n",
    "pl.fit(X_train, y_train)\n",
    "print(pl.predict(X_test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Pipeline\n",
    "You'll now create the Pipeline again, but directly, skipping the step of initializing the StandardScaler and LogisticRegression as a variable. Instead, you will do the initialization as part of the Pipeline creation.\n",
    "\n",
    "You'll then store the model for further use.\n",
    "\n",
    "The data is available as X_train, with the labels as y_train.\n",
    "\n",
    "StandardScaler, LogisticRegression and Pipeline have been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipeline\n",
    "pl = Pipeline([\n",
    "        (\"scale\", StandardScaler()),\n",
    "        (\"logreg\", LogisticRegression())\n",
    "    ])\n",
    "\n",
    "# Fit the pipeline\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Store the model\n",
    "with Path(\"pipeline.pkl\").open('bw') as f:\n",
    "\tpickle.dump(pl, f)\n",
    "  \n",
    "# Load the pipeline\n",
    "with Path(\"pipeline.pkl\").open('br') as f:\n",
    "\tpl_loaded = pickle.load(f)\n",
    "\n",
    "print(pl_loaded)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply model to data stream\n",
    "Let's now apply your trained machine learning Pipeline to streaming data, and categorize the values immediately.\n",
    "\n",
    "You'll then use predict() on the incoming messages to determine the category. Based on the result of the prediction you will take action, and close the windows in your house (or not).\n",
    "\n",
    "Remember that category 1 means good weather, whereas category 0 signifies bad, cold weather.\n",
    "\n",
    "Additionally, the pipeline returns an array of predictions. As you passed in only one element, you need to access the first element using category[0].\n",
    "\n",
    "The function close_window() will handle this for you, and will additionally log the record for further study.\n",
    "\n",
    "pandas as pd and json have been preloaded the session for you, and the model is available as pl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_subscribe(client, userdata, message):\n",
    "    data = json.loads(message.payload)\n",
    "    # Parse to DataFrame\n",
    "    df = pd.DataFrame.from_records([data], index='timestamp', columns=cols)\n",
    "    # Predict result\n",
    "    category = pl.predict(df)\n",
    "    if category[0] < 1:\n",
    "        # Call business logic\n",
    "        close_window(df, category)\n",
    "    else:\n",
    "        print(\"Nice Weather, nothing to do.\")  \n",
    "\n",
    "# Subscribe model_subscribe to MQTT Topic\n",
    "subscribe.callback(model_subscribe, topic, hostname=MQTT_HOST)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
